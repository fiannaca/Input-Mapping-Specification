\documentclass{article}

\usepackage[utf8]{inputenc}
\usepackage[margin=1in]{geometry}
\usepackage{fancyhdr}
\usepackage{lastpage}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{fancyvrb}
\usepackage{hyperref}
\usepackage{url}
\usepackage{color,soul}
\usepackage{graphicx}

\newcommand{\Z}{\mathbb{Z}}
\newcommand{\R}{\mathbb{R}}
\newtheorem{define}{Definition}

\fancypagestyle{firststyle}
{
    \fancyhf{}
    \lhead{\textbf{Input Mapping Specification}}
    \rhead{\textbf{Alex Fiannaca}}
    \cfoot{\thepage\ of \pageref{LastPage}}
}
\pagestyle{firststyle}

\linespread{2}

\title{Input Mapping Specification}

\begin{document}

\maketitle
\thispagestyle{firststyle}

\hl{Note: This is currently too verbose.
I'm working on clarifying my ideas and working down to the most concise and clear description possible.}

At a high level of abstraction, it appears that computer input can be grouped into two sets: discrete input devices such as keyboards and continuous input devices such as mouses and microphones.
While this may be a valid interpretation when considering the end effect of these input devices (character injection, 2D cursor movement, and audio capture respectively), it abstracts away how computers actually receive and handle input. In the context of Windows, all Human Input Device (HID) data is provided to the computer as discretized raw input packages. As a concrete example, when the mouse is moved, the Windows HID drivers broadcast a packet containing the offset of the mouse from its previous location. Therefore, what appeared to be continuous type input consisting of a path the mouse moved is actually observed by the computer as a series of events, each of which indicates a single offset value. This low level view of computer input as a time-discretized series of input events allows us to generate a unified model of computer input and a language for describing reactive mappings from a series of input events to system actions. In the following sections, we will explore the primitives of this computer input model and build up from these primitives using recent work in functional reactive programming in order to define a language for specifying mappings between input devices and their realized interaction effect within the corresponding interface.

\hl{\tiny Maya says: This makes sense, but my first reaction is that we might be loosing some functionality if we think purely in terms of events; in other words by being purely reactive as opposed to proactively checking the state of an input the device. For example at any time you can check the position of the mouse cursor and map that to something rather than wait for it to move.}

\section{Modeling Device Input and Units of Interaction}

\begin{define}
\label{def:event}
An \textbf{input event} is a unit of state information being provided to the system from an input device at a discrete time step.
\end{define}

Typically ``input events'' are thought of as referring to a series of one or more state changes within a fixed time constraint, thereby describing a unit of interaction in an interface (e.g. a \textit{click} event).
In this work, we build off of the concepts and terminology of Event-Driven Functional Reactive Programming (E-FRP) as originally developed by Wan et al. \cite{wan2002event}. From this perspective, our model of device input reframes this terminology in a more fine grained manner, as seen in Definition \ref{def:event}. Essentially, we take a view of input events as externally sourced stimuli provided to the computer in discrete time steps. This has an important implication in modeling device input and units of interaction: input events are not necessarily units of interaction. For instance, \textit{click}, a standard unit of interaction in many interfaces, is not represented by a single input event in this model. Instead, it is represented by a sequence of input events (\texttt{lmouse\_btn\_evt.value.state == DOWN, lmouse\_btn\_evt.value.state == UP}) meeting particular timing constraints (usually both events for \texttt{DOWN} and \texttt{UP} must occur within a time limit on the order of several hundred milliseconds). Due to this distinction between input events and units of interaction in our model, we refer to a unit of interaction as a \textbf{gesture} (we will defined this in a more concrete manner shortly). The benefit of viewing device input and units of interaction in this manner is that it both allows us to describe input that the system already expects (clicks, key presses, etc) in addition to describing structures of input that the computer does not already expect (new touch gestures, switch-based input encodings, etc).

\hl{\tiny Maya says: Are we sure gesture is the right word for this? Given its meanings that immediately come to mind (as in ``touch gesture''), my brain is rejecting this general meaning. Perhaps ``event sequence'' or ``sequence'' or ``interaction unit?''}

The time-discretized nature of this E-FRP based input model allows us to consider all computer input as being state-based since at each time step, the most recent input event reflects the system's belief of the current state of the input device.
As an example, if at time $t$ keyboard key $k$ is pressed, then the state of $k$, denoted $S_k(t)$, is $[1]$, reflecting the key's ``DOWN'' state (as opposed to $0$ for the key's ``UP'' state). As a second example, if at time $t$ the mouse $m$ is moving up 5 units and to the right 7 units, then $S_m(t) = [7, 5]$. These two examples differ in only one respect: in the first example $S_k$ maps from a time $t$ to a vector from a subset of $\Z$ (specifically, the set $\{0, 1\}$), whereas in the second example $S_m$ maps from a time $t$ to a vector from a subset of $\Z^2$ (bounded in each dimension by the maximum offset detectable by the mouse in a single timestep in positive and negative x and y directions respectively). This provides an intuition into our model's definition for generalized computer input:

\begin{define}
\textbf{Input} from device $d$ is defined as a \textbf{stream} of \textbf{input events} each representing the state of device $d$ at some time $t$, written as $S_d(t)$, where $S_d(t)$ maps time $t$ to a subset of $\Z^n$ with $n$ being device dependent.
\end{define}

\hl{\tiny Maya says: I'm resisting the definition of an input being a ``mapping" from time (t) to a value. Similarly a device being equivalent to a time series. I think the device is the ``process'' that generates the events. The input is an ordered sequence of asynchronous events, not really a time series. }

\hl{Note: I have yet to find any devices that provide floating point values for input, but I'm sure they exist, so maybe this should be ``a subset of $\R^n$'' instead...}

In order to capture the relation between input events and gestures, our model adopts the semantics of E-FRP as implemented in the reactive Vega visualization grammar \cite{satyanarayan2014declarative} with several modifications.
These semantics are based on the concept of \textbf{\textit{streams}} and \textbf{\textit{signals}}. {\em Streams} are time ordered sequences (mutually exclusive) of input events. {\em Signals} are arbitrary reactive expressions composed from input events values (denoted $R(S_d(t))$). Streams represent input events as ordered data flows which can be operated over via filtering and combining operations. Signals are computed by evaluating the signal's reactive expression over the state of the most recent event in the stream that the signal corresponds to ($S_d(t)$), utilizing the fact that streams are treated as first class data sources. 

\hl{\tiny Maya says: Definition of stream and event is not clear here. Also, what is the difference between stream and gesture? Isn't signal equivalent to state?}

Given these primitives (signals and streams), we can now offer a formal definition for a gesture:

\begin{define}
\label{def:gesture}
A \textbf{gesture} $g$, is a function $f(R, T)$ over the previous $n$ values and times of a signal (i.e. $R = [R(S_d(t-n)), R(S_d(t-n+1)), ..., R(S_d(t))]$, and $T = [t - n + 1, ... , t]$) which returns a tuple consisting of a boolean value indicating if the gesture occurred, and a dictionary of parameter values of the gesture.
Each gesture is associated to one or more actions $a$ which are executed when $f(R,T)$ returns the boolean value $true$. Gestures are the base unit of interaction within an interface.
\end{define}

\hl{\tiny Maya says: Shouldn't gesture association with action be separate from whether or not the gesture occurred?}

\hl{Note: This definition is overly complex.
I need to think of a more concise and simple way to state it.}

\noindent
In many simple cases, gesture $g$ is simply a function of only the most recent value of a signal which always evaluates to true and returns as a parameter the value of its corresponding signal, thereby executing an action whenever the signal occurs and passing the signal value along to an action.
As an example, consider a gesture called \textit{mousemove\_gesture} which captures any movement of the mouse. This gesture only considers the most recent \textit{mousemove} signal, reactively executing its associated action \textit{movecursor} whenever the \textit{mousemove} signal has a value. In more complex cases, gesture $g$ can implement a more expressive recognizer such as machine learning-based gesture recognizer. Given the flexibility of this model, gestures can capture simple units of interaction such as a mouse click or a button press, or very complex units of interaction such as touch gestures on touch pad, and everything in between.

Next, we define mappings $M: g \rightarrow a$, as the assignment of an action to a gesture such that actions are executed reactively at the occurrence of a gesture.
As an example, a simple mapping could map from the \textit{btn\_x\_pressed} gesture to the \textit{movecursor} action and would therefore react to a gesture that captures the 'x' keyboard key being pressed by executing the \textit{movecursor} action, allowing a keyboard input event to act as a mouse input event. It is important to note that there are a set of of gestures and actions defined by default in this model. These defaults correspond to the standard ``input events'' of the desktop paradigm: mousemove, button\_click, keypress, etc. 

In order to better understand what actions represent in this model, let us properly define an action:

\begin{define}
\label{def:action}
An \textbf{action} $a$ effects some form of interaction with the interface.
Actions can effect interaction at one of three levels: \textbf{OS Raw Input Level}, \textbf{OS Command Level}, \textbf{Application Interface Level}. OS Raw Input actions provide the system with ``expected'' input such as keyboard input or mouse input. OS Command actions provide the system with commands that can be executed at a command prompt. Finally, Application Interface actions directly interact with software \textbf{agents}.
\end{define}

\begin{define}
\label{def:agent}
A software \textbf{agent} is a program which advertises the different parts of its human interface which can be interacted with to our system.
This allows actions to directly effect high level interaction with an interface for a software application. The parts of an agent's interface which can be interacted with are called \textbf{interactors}. Interactors may change state (i.e. availability: whether or not triggering the interactor will have any affect) over time.
\end{define}


In the next section, we discuss the syntax of a domain specific language called MapAll, based on this model, which allows us to declaratively define streams, signals, gestures, and actions using a variation of the reactive Vega syntax.
A key feature of this language is that it allows for the remapping of default gestures to non-default actions in addition to the creation of new gestures mapping to either new actions or any of the predefined default actions.

% \hl{(, and takes on the value of the offset parameters of this signal)}

\section{The MapAll Language}

The MapAll declarative language is an embedded language in JSON consisting of four main blocks: \texttt{signals}, \texttt{gestures}, \texttt{actions}, \hl{and \texttt{mappings}}.
Note that there is not a stream block due to the fact that streams are automatically defined corresponding to each of the possible input events ("lmouse\_btn\_evt", "mouse\_move\_evt", "keyboard\_evt", etc - see previous document breakig down all of the possible forms of input). The following sections will describe each of the four main blocks of MapAll in turn.

\hl{Note: I still need to add language support for agents.
This means I also need to create an API for how agents advertise their interactors.}

\subsection{The \texttt{signals} Block}
Syntax:

\begin{Verbatim}[baselinestretch=1.0]
{
    "signals": [
        {
            "name": "<signal_name>",
            "init": <initial_value_object>,
            "streams": [
                {
                    "type": "<stream_expression>",
                    "expr": "<reactive_expression>"
                }, ...
            ]
        }, ...
    ]
}
\end{Verbatim}

The \texttt{signals} block consists of a list of signals that the program can react to.
Each signal consists of a name, and initial value, and a series of streams it combines. Each stream is specified with a type and an arbitrary reactive expression. The type field is an expression over signals and streams with can combine, filter, and synthesize other streams. When an event occurs on the resulting stream, the reactive expression is evaluated to set the signal's value. In the example below, two signals are created that react to events occurring on the left mouse button stream. The first signal becomes true when the left mouse button goes to the state DOWN, and the second signal becomes true when the left mouse button goes to the state UP. Note that for raw streams, the field \texttt{value} corresponds to the information received in each HID packet by the system, while for signals built on top of signals, \texttt{value} corresponds to the type returned by the \texttt{expr} of its constituent signals. Additionally, the \texttt{value} field always contains the time $t$ at which the event occurred, regardless of the type of \texttt{expr}.

\begin{Verbatim}[baselinestretch=1.0]
    {
        "name": "left_mousedown",
        "init": false,
        "streams": [{
            "type": "lmouse_btn_evt",
            "expr": "value.state == DOWN"
        }]
    },
    {
        "name": "left_mouseup",
        "init": false,
        "streams": [{
            "type": "lmouse_btn_evt",
            "expr": "value.state == UP"
        }]
    }
\end{Verbatim}

The next example demonstrates two critical operations which can be performed over streams: event synthesis, and time constraint filtering.
In event synthesis, multiple events occurring in a particular order (not necessarily sequentially) are grouped with \texttt{()} into a new super event. In time constraint filtering, grouped events are filtered by the minimum and maximum time in which they must all occur. The example below show how the \texttt{left\_mousedown} and \texttt{left\_mouseup} signals can be combined to capture a click. Note that in synthesis, the resulting \texttt{value} field consists of the union of all properties of each constituent event's \texttt{value} field.

\begin{Verbatim}[baselinestretch=1.0]
    {
        "name": "click",
        "init": {},
        "streams": [{
            "type": "(left_mousedown, left_mouseup){0, 500}"
            "expr": "value"
        }]
    }
\end{Verbatim}

Another way in which grouping may occur is by using bounding events.
Bounding events (specified with \texttt{[]}) select all events which occur between two events (in the case of the example below: left\_mousedown and left\_mouseup). The \texttt{>} operator selects all children of a particular type from a set of bounded events. In this example, the signal \texttt{drag} captures all of the mouse movement events which occur between a left\_mousedown signal and a left\_mouseup signal:

\begin{Verbatim}[baselinestretch=1.0]
    {
        "name": "drag",
        "init": {"x": 0, "y": 0},
        "streams": [{
            "type": "[left_mousedown, left_mouseup] > mouse_move_evt"
            "expr": "value"
        }]
    }
\end{Verbatim}

Importantly, streams can be filtered based on the property values of their constituent events:

\begin{Verbatim}[baselinestretch=1.0]
    {
        "name": "btn_x_down",
        "init": false,
        "streams": [{ 
            "type": "keyboard_evt[value.code == 'x'][value.state = DOWN]",
            "expr": "value"
        }]
    },
    {
        "name": "btn_x_up",
        "init": false,
        "streams": [{ 
            "type": "keyboard_evt[value.code == 'x'][value.state = UP]",
            "expr": "value"
        }]
    },
    {
        "name": "btn_x_pressed",
        "init": {},
        "streams": [{ 
            "type": "(btn_x_down, btn_x_up)",
            "expr": "value"
        }]
    }
\end{Verbatim}

\subsection{The \texttt{actions} Block}
Syntax:

\begin{Verbatim}[baselinestretch=1.0]
{
    "actions": [
        {
            "name": "<action_name>",
            "args": [ /* OPTIONAL */
                { 
                    "arg": "<named_arg>"
                }, ...
            ],
            "input": [ /* OPTIONAL - At least one input or command item must exist */
                {
                    "type": "<standard_input_type>",
                    "value": "<input_value>",
                    "order": "<optional_order>"
                }, ...
            ],
            "command": [ /* OPTIONAL - At least one input or command item must exist */
                {
                    "program": "<program_to_run>",
                    "options": "<program_options>",
                    "order": "<optional_order>"
                }, ...
            ]
        }, ...
    ]
}
\end{Verbatim}

Actions represent the effect you want a gesture to have.
This can be grouped into either standard (``expected'') input, or commands line items to be executed (opening a file, starting a program). Actions can entail one or more standard input item, one or more command item, or any combination therein. If an action has more than one input and/or command item, items will be executed in the order they were declared by default. This order can be altered by specifying and order parameter, allowing command items to be interleaved with input items.

\hl{Examples need to be added to this section...}

\subsection{The \texttt{gestures} Block}
Syntax:

\begin{Verbatim}[baselinestretch=1.0]
{
    "gestures": [
        {
            "name": "<gesture_name>",
            "signal": "<signal_name>",
            "action": "<action_expression>",
            "action_args": [
                {
                    "arg": "<arg_name>",
                    "value": <arg_value>
                }, ...
            ],
            "consume": <boolean>, /* OPTIONAL */
            "steps": <integer>, /* OPTIONAL */
            "recognizer": "<user_defined_function>" /* OPTIONAL */
        }, ...
    ]
}
\end{Verbatim}

\noindent
\hl{\textbf{Issues}: This section still has many open questions: 1) How should we specify the values a gesture can return in its parameters? 2) How should input consumption work? 3) Is a separate ``mappings'' block necessary if each gesture already declares what action it is mapped to? and likewise, should gestures declare the action they are mapped to?}

In the most basic case (and by far the most common case), a gesture can simply connect a signal directly to an action.
This can be specified by adding a entry to the \texttt{gestures} block:

\begin{Verbatim}[baselinestretch=1.0]
    {
        "name": "click_gesture",
        "signal": "click",
        "action": "click_action"
    }
\end{Verbatim}

\hl{Edit this paragraph - consider the implications of consuming or not consuming input.
Should input only be consumed when a gesture-action mapping is defined?} When "consume" is set to true, the system will forget the historical values of the associated signal at the next time step (i.e. when future gestures are evaluated starting at time $(t+1)$, it will appear that time started at $(t+1)$ and the associated signals to not yet have a history). Note that in the example above, "consume" is optional since the default value of "consume" is false.

In the more interesting (but more obscure) case, gestures may consider the values of a signal over more than one time step.
This is accomplished by providing the system with a user defined boolean function that takes a sequence of signal values and returns true if the sequence corresponds to a particular gesture or false otherwise. Additionally, the maximum number of time steps of values provided to the user defined function must be specified in the gesture block:

\begin{Verbatim}[baselinestretch=1.0]
    {
        "name": "morse_code",
        "signal": "btn_x_pressed"
        "steps": 10
        "recognizer": "user_defined_func"
        "action": "some_action"
    }
\end{Verbatim}

\subsection{The \texttt{mappings} Block}
Syntax:

\begin{Verbatim}[baselinestretch=1.0]
{
    "mappings": [
        {
            "gesture": "<gesture_name>",
            "action": "<new_action>"
        }, ...
    ]
}
\end{Verbatim}

\hl{In the current layout of the MapAll language there is something of a battle between what level of abstraction different events are define at.
For instance, a click is defined using signals and doesn't really require a gesture definition since the gesture only connects it to an action (a functionality which could be carried out in the ``mappings'' block). On the other hand, if the mappings block weren't defined, then a gesture would be necessary to connect the click signal to its action (executing the standard input click event). More work needs to be done to clear up what level of abstraction things should happen at by default.}

\bibliographystyle{abbrv}
\bibliography{references.bib} 

\end{document}
